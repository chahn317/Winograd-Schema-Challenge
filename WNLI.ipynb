{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports/Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "if [[ ! -d \"./data\" ]]\n",
    "then\n",
    "  echo \"Downloading files if missing\"\n",
    "  git clone https://github.com/kabirahuja2431/CSE447-547MAutumn2024.git\n",
    "  cp -r ./CSE447-547MAutumn2024/\"Project 2\"/data .\n",
    "  cp ./CSE447-547MAutumn2024/\"Project 2\"/wordvec_tests.py .\n",
    "  cp ./CSE447-547MAutumn2024/\"Project 2\"/nn_tests.py .\n",
    "  cp ./CSE447-547MAutumn2024/\"Project 2\"/glove.py .\n",
    "  cp ./CSE447-547MAutumn2024/\"Project 2\"/siqa.py .\n",
    "  wget https://homes.cs.washington.edu/~kahuja/cse447/project2/glove.6B.50d.txt -O data/embeddings/glove.6B/glove.6B.50d.txt\n",
    "  wget https://homes.cs.washington.edu/~kahuja/cse447/project2/X_train_st.pt -O data/sst/X_train_st.pt\n",
    "  wget https://homes.cs.washington.edu/~kahuja/cse447/project2/X_dev_st.pt -O data/sst/X_dev_st.pt\n",
    "  wget https://homes.cs.washington.edu/~kahuja/cse447/project2/train_data_embedded.pt -O data/socialiqa-train-dev/train_data_embedded.pt\n",
    "  wget https://homes.cs.washington.edu/~kahuja/cse447/project2/dev_data_embedded.pt -O data/socialiqa-train-dev/dev_data_embedded.pt\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /Users/christinahahn/anaconda3/envs/cse446/lib/python3.9/site-packages (1.2.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Users/christinahahn/anaconda3/envs/cse446/lib/python3.9/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /Users/christinahahn/anaconda3/envs/cse446/lib/python3.9/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: numpy>=1.16.5 in /Users/christinahahn/anaconda3/envs/cse446/lib/python3.9/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: six>=1.5 in /Users/christinahahn/anaconda3/envs/cse446/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas) (1.16.0)\n",
      "Requirement already satisfied: sentence-transformers in /Users/christinahahn/anaconda3/envs/cse446/lib/python3.9/site-packages (3.3.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /Users/christinahahn/anaconda3/envs/cse446/lib/python3.9/site-packages (from sentence-transformers) (4.46.3)\n",
      "Requirement already satisfied: tqdm in /Users/christinahahn/anaconda3/envs/cse446/lib/python3.9/site-packages (from sentence-transformers) (4.66.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /Users/christinahahn/anaconda3/envs/cse446/lib/python3.9/site-packages (from sentence-transformers) (2.5.1)\n",
      "Requirement already satisfied: scikit-learn in /Users/christinahahn/anaconda3/envs/cse446/lib/python3.9/site-packages (from sentence-transformers) (1.2.2)\n",
      "Requirement already satisfied: scipy in /Users/christinahahn/anaconda3/envs/cse446/lib/python3.9/site-packages (from sentence-transformers) (1.10.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /Users/christinahahn/anaconda3/envs/cse446/lib/python3.9/site-packages (from sentence-transformers) (0.26.2)\n",
      "Requirement already satisfied: Pillow in /Users/christinahahn/anaconda3/envs/cse446/lib/python3.9/site-packages (from sentence-transformers) (10.0.1)\n",
      "Requirement already satisfied: filelock in /Users/christinahahn/anaconda3/envs/cse446/lib/python3.9/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/christinahahn/anaconda3/envs/cse446/lib/python3.9/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/christinahahn/anaconda3/envs/cse446/lib/python3.9/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/christinahahn/anaconda3/envs/cse446/lib/python3.9/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in /Users/christinahahn/anaconda3/envs/cse446/lib/python3.9/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/christinahahn/anaconda3/envs/cse446/lib/python3.9/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.9.0)\n",
      "Requirement already satisfied: networkx in /Users/christinahahn/anaconda3/envs/cse446/lib/python3.9/site-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/christinahahn/anaconda3/envs/cse446/lib/python3.9/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Users/christinahahn/anaconda3/envs/cse446/lib/python3.9/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/christinahahn/anaconda3/envs/cse446/lib/python3.9/site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/christinahahn/anaconda3/envs/cse446/lib/python3.9/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/christinahahn/anaconda3/envs/cse446/lib/python3.9/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /Users/christinahahn/anaconda3/envs/cse446/lib/python3.9/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/christinahahn/anaconda3/envs/cse446/lib/python3.9/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/christinahahn/anaconda3/envs/cse446/lib/python3.9/site-packages (from scikit-learn->sentence-transformers) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/christinahahn/anaconda3/envs/cse446/lib/python3.9/site-packages (from scikit-learn->sentence-transformers) (3.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/christinahahn/anaconda3/envs/cse446/lib/python3.9/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/christinahahn/anaconda3/envs/cse446/lib/python3.9/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/christinahahn/anaconda3/envs/cse446/lib/python3.9/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/christinahahn/anaconda3/envs/cse446/lib/python3.9/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/christinahahn/anaconda3/envs/cse446/lib/python3.9/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.8.30)\n",
      "Collecting tf_keras\n",
      "  Downloading tf_keras-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: tensorflow<2.19,>=2.18 in /Users/christinahahn/anaconda3/envs/cse446/lib/python3.9/site-packages (from tf_keras) (2.18.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Users/christinahahn/anaconda3/envs/cse446/lib/python3.9/site-packages (from tensorflow<2.19,>=2.18->tf_keras) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/christinahahn/anaconda3/envs/cse446/lib/python3.9/site-packages (from tensorflow<2.19,>=2.18->tf_keras) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /Users/christinahahn/anaconda3/envs/cse446/lib/python3.9/site-packages (from tensorflow<2.19,>=2.18->tf_keras) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /Users/christinahahn/anaconda3/envs/cse446/lib/python3.9/site-packages (from tensorflow<2.19,>=2.18->tf_keras) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/christinahahn/anaconda3/envs/cse446/lib/python3.9/site-packages (from tensorflow<2.19,>=2.18->tf_keras) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/christinahahn/anaconda3/envs/cse446/lib/python3.9/site-packages (from tensorflow<2.19,>=2.18->tf_keras) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/christinahahn/anaconda3/envs/cse446/lib/python3.9/site-packages (from tensorflow<2.19,>=2.18->tf_keras) (3.4.0)\n",
      "Requirement already satisfied: packaging in /Users/christinahahn/anaconda3/envs/cse446/lib/python3.9/site-packages (from tensorflow<2.19,>=2.18->tf_keras) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /Users/christinahahn/anaconda3/envs/cse446/lib/python3.9/site-packages (from tensorflow<2.19,>=2.18->tf_keras) (5.28.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/christinahahn/anaconda3/envs/cse446/lib/python3.9/site-packages (from tensorflow<2.19,>=2.18->tf_keras) (2.31.0)\n",
      "Requirement already satisfied: setuptools in /Users/christinahahn/anaconda3/envs/cse446/lib/python3.9/site-packages (from tensorflow<2.19,>=2.18->tf_keras) (59.8.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/christinahahn/anaconda3/envs/cse446/lib/python3.9/site-packages (from tensorflow<2.19,>=2.18->tf_keras) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/christinahahn/anaconda3/envs/cse446/lib/python3.9/site-packages (from tensorflow<2.19,>=2.18->tf_keras) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/christinahahn/anaconda3/envs/cse446/lib/python3.9/site-packages (from tensorflow<2.19,>=2.18->tf_keras) (4.9.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Users/christinahahn/anaconda3/envs/cse446/lib/python3.9/site-packages (from tensorflow<2.19,>=2.18->tf_keras) (1.17.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/christinahahn/anaconda3/envs/cse446/lib/python3.9/site-packages (from tensorflow<2.19,>=2.18->tf_keras) (1.68.0)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in /Users/christinahahn/anaconda3/envs/cse446/lib/python3.9/site-packages (from tensorflow<2.19,>=2.18->tf_keras) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in /Users/christinahahn/anaconda3/envs/cse446/lib/python3.9/site-packages (from tensorflow<2.19,>=2.18->tf_keras) (3.6.0)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /Users/christinahahn/anaconda3/envs/cse446/lib/python3.9/site-packages (from tensorflow<2.19,>=2.18->tf_keras) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /Users/christinahahn/anaconda3/envs/cse446/lib/python3.9/site-packages (from tensorflow<2.19,>=2.18->tf_keras) (3.12.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /Users/christinahahn/anaconda3/envs/cse446/lib/python3.9/site-packages (from tensorflow<2.19,>=2.18->tf_keras) (0.4.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /Users/christinahahn/anaconda3/envs/cse446/lib/python3.9/site-packages (from tensorflow<2.19,>=2.18->tf_keras) (0.37.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/christinahahn/anaconda3/envs/cse446/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow<2.19,>=2.18->tf_keras) (0.42.0)\n",
      "Requirement already satisfied: rich in /Users/christinahahn/anaconda3/envs/cse446/lib/python3.9/site-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18->tf_keras) (13.7.0)\n",
      "Requirement already satisfied: namex in /Users/christinahahn/anaconda3/envs/cse446/lib/python3.9/site-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18->tf_keras) (0.0.7)\n",
      "Requirement already satisfied: optree in /Users/christinahahn/anaconda3/envs/cse446/lib/python3.9/site-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18->tf_keras) (0.13.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/christinahahn/anaconda3/envs/cse446/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18->tf_keras) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/christinahahn/anaconda3/envs/cse446/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18->tf_keras) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/christinahahn/anaconda3/envs/cse446/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18->tf_keras) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/christinahahn/anaconda3/envs/cse446/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18->tf_keras) (2024.8.30)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/christinahahn/anaconda3/envs/cse446/lib/python3.9/site-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf_keras) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/christinahahn/anaconda3/envs/cse446/lib/python3.9/site-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf_keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/christinahahn/anaconda3/envs/cse446/lib/python3.9/site-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf_keras) (3.1.3)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /Users/christinahahn/anaconda3/envs/cse446/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf_keras) (7.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/christinahahn/anaconda3/envs/cse446/lib/python3.9/site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf_keras) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/christinahahn/anaconda3/envs/cse446/lib/python3.9/site-packages (from rich->keras>=3.5.0->tensorflow<2.19,>=2.18->tf_keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/christinahahn/anaconda3/envs/cse446/lib/python3.9/site-packages (from rich->keras>=3.5.0->tensorflow<2.19,>=2.18->tf_keras) (2.17.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/christinahahn/anaconda3/envs/cse446/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf_keras) (3.17.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/christinahahn/anaconda3/envs/cse446/lib/python3.9/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow<2.19,>=2.18->tf_keras) (0.1.2)\n",
      "Downloading tf_keras-2.18.0-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tf_keras\n",
      "Successfully installed tf_keras-2.18.0\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Install required packages\n",
    "pip install pandas\n",
    "pip install sentence-transformers\n",
    "pip install tf_keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/christinahahn/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/christinahahn/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from typing import List, Dict\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {DEVICE} device\")\n",
    "\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_dir = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "data_dir = os.path.join(parent_dir, \"data\")\n",
    "wnli_dir = os.path.join(parent_dir, \"WNLI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WNLI Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(f'{wnli_dir}/train.tsv', sep='\\t')\n",
    "dev_df = pd.read_csv(f'{wnli_dir}/dev.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glove Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GloveEmbeddings:\n",
    "\n",
    "    def __init__(self, path=\"embeddings/glove.6B/glove.6B.50d.txt\"):\n",
    "        \"\"\"\n",
    "        Initializes GloveEmbeddings object.\n",
    "\n",
    "        Inputs:\n",
    "        - path: The path to the GloVe embedding data\n",
    "        \n",
    "        \"\"\"\n",
    "        self.path = path\n",
    "        self.vec_size = int(re.search(r\"\\d+(?=d)\", path).group(0))\n",
    "        self.embeddings = {}\n",
    "        self.load()\n",
    "\n",
    "    def load(self):\n",
    "        \"\"\"\n",
    "        Loads the GloVe embedding data.\n",
    "        \n",
    "        \"\"\"\n",
    "        for line in open(self.path, \"r\"):\n",
    "            values = line.split()\n",
    "\n",
    "            word_len = len(values) - self.vec_size\n",
    "\n",
    "            word = \" \".join(values[:word_len])\n",
    "            vector_values = list(map(float, values[word_len:]))\n",
    "\n",
    "            word = values[0]\n",
    "            vector_values = list(map(float, values[-self.vec_size :]))\n",
    "            vector = torch.tensor(vector_values, dtype=torch.float)\n",
    "            self.embeddings[word] = vector\n",
    "\n",
    "    def is_word_in_embeddings(self, word):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - word: The word to search for\n",
    "\n",
    "        Returns:\n",
    "        - bool: True if word is in the GloVe embedding data, false otherwise\n",
    "        \n",
    "        \"\"\"\n",
    "        return word in self.embeddings\n",
    "\n",
    "    def get_vector(self, word):\n",
    "        if not self.is_word_in_embeddings(word):\n",
    "            return self.embeddings[\"unk\"]\n",
    "        return self.embeddings[word]\n",
    "\n",
    "    def __getitem__(self, word):\n",
    "        return self.get_vector(word)\n",
    "\n",
    "glove_embeddings = GloveEmbeddings(\n",
    "    path=f\"{data_dir}/embeddings/glove.6B/glove.6B.50d.txt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining sentence embedding function for GloveEmbeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_embedding(\n",
    "    sentence: str,\n",
    "    word_embeddings: GloveEmbeddings,\n",
    "    use_POS: bool = False,\n",
    "    pos_weights: Dict[str, float] = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute the sentence embedding using the word embeddings.\n",
    "\n",
    "    Inputs:\n",
    "    - sentence: The input sentence\n",
    "    - word_embeddings: GloveEmbeddings object\n",
    "    - use_POS: Whether to use POS tagging\n",
    "    - pos_weights: Dictionary containing POS weights\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: The sentence embedding\n",
    "    \"\"\"\n",
    "    tokens = word_tokenize(sentence.lower())\n",
    "\n",
    "    if use_POS:\n",
    "        tags = nltk.pos_tag(tokens)\n",
    "        embeddings = []\n",
    "        for w, t in tags:\n",
    "            emb = torch.zeros(word_embeddings.vec_size)\n",
    "            if word_embeddings.is_word_in_embeddings(w) and t in pos_weights:\n",
    "                emb = word_embeddings[w] * pos_weights[t]\n",
    "            embeddings.append(emb)\n",
    "    else:\n",
    "        embeddings = [word_embeddings[w] for w in tokens if word_embeddings.is_word_in_embeddings(w)]\n",
    "\n",
    "    if embeddings:\n",
    "        return torch.sum(torch.stack(embeddings), dim=0)\n",
    "    else:\n",
    "        return torch.zeros((word_embeddings.vec_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining sentence embedding function for SentenceTransformer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_st_embeddings(\n",
    "    sentences: List[str],\n",
    "    st_model: SentenceTransformer,\n",
    "    batch_size: int = 32,\n",
    "    device: str = \"cpu\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute the sentence embedding using the Sentence Transformer model.\n",
    "\n",
    "    Inputs:\n",
    "    - sentence: The input sentence\n",
    "    - st_model: SentenceTransformer model\n",
    "    - batch_size: Encode in batches to avoid memory issues in case multiple sentences are passed\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: The sentence embedding of shape [d,] (when only 1 sentence) or [n, d] where n is the number of sentences and d is the embedding dimension\n",
    "    \"\"\"\n",
    "\n",
    "    st_model.to(device)\n",
    "    sentence_embeddings = None\n",
    "\n",
    "    for i in range(0, len(sentences), batch_size):\n",
    "        batch_sentences = sentences[i : i + batch_size]\n",
    "        batch_embeddings = st_model.encode(batch_sentences, convert_to_tensor=True)\n",
    "        if sentence_embeddings is None:\n",
    "            sentence_embeddings = batch_embeddings\n",
    "        else:\n",
    "            sentence_embeddings = torch.cat(\n",
    "                [sentence_embeddings, batch_embeddings], dim=0\n",
    "            )\n",
    "\n",
    "    return sentence_embeddings.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embed WNLI Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_wnli(embed_method, df):\n",
    "    \"\"\"\n",
    "    Preprocesses data.\n",
    "\n",
    "    Inputs:\n",
    "    - embed_method: Either \"glove\" to use GloVe or \"st\" to use Sentence Transformers\n",
    "    - df: Pandas dataframe\n",
    "\n",
    "    Returns:\n",
    "    List[Dict[str, torch.Tensor]]: Embeddings specified by embed_method\n",
    "    \"\"\"\n",
    "    if embed_method == \"glove\":\n",
    "        s1 = torch.stack([get_sentence_embedding(s, glove_embeddings, use_POS=False) \n",
    "                                for s in df[\"sentence1\"].values])\n",
    "        s2 = torch.stack([get_sentence_embedding(s, glove_embeddings, use_POS=False) \n",
    "                                for s in df[\"sentence2\"].values])\n",
    "    elif embed_method == \"st\":\n",
    "        st_model = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "        s1 = get_st_embeddings(df[\"sentence1\"].values, st_model, device=DEVICE)\n",
    "        s2 = get_st_embeddings(df[\"sentence2\"].values, st_model, device=DEVICE)\n",
    "\n",
    "    return [{\"sentence1\": a, \"sentence2\": b} for a, b in zip(s1, s2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_glove = preprocess_wnli(\"glove\", train_df)\n",
    "X_dev_glove = preprocess_wnli(\"glove\", dev_df)\n",
    "X_train_st = preprocess_wnli(\"st\", train_df)\n",
    "X_dev_st = preprocess_wnli(\"st\", dev_df)\n",
    "\n",
    "Y_train = torch.Tensor(train_df[\"label\"].values)\n",
    "Y_dev = torch.Tensor(dev_df[\"label\"].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WNLIEmbeddedDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, embeddings: List[Dict[str, torch.Tensor]], labels: List[str]):\n",
    "        self.embeddings = embeddings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.embeddings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.embeddings[idx]\n",
    "        return {\n",
    "            \"sentence1\": sample[\"sentence1\"],\n",
    "            \"sentence2\": sample[\"sentence2\"],\n",
    "            \"label\": self.labels[idx]\n",
    "        }\n",
    "\n",
    "def get_wnli_dataloader(\n",
    "    embeddings: List[Dict[str, torch.Tensor]],\n",
    "    labels: List[str],\n",
    "    batch_size: int = 32,\n",
    "    shuffle: bool = True,\n",
    "):\n",
    "\n",
    "    dataset = WNLIEmbeddedDataset(embeddings, labels)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "train_glove_dataloader = get_wnli_dataloader(X_train_glove, Y_train, batch_size=32, shuffle=True)\n",
    "train_st_dataloader = get_wnli_dataloader(X_train_st, Y_train, batch_size=32, shuffle=True)\n",
    "dev_glove_dataloader = get_wnli_dataloader(X_dev_glove, Y_dev, batch_size=32, shuffle=False)\n",
    "dev_glove_dataloader = get_wnli_dataloader(X_dev_st, Y_dev, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WNLIFFNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim: int, hidden_dim: int, depth = 1):\n",
    "        super(WNLIFFNN, self).__init__()\n",
    "\n",
    "        layers = [nn.Linear(input_dim * 2, hidden_dim), nn.ReLU()]\n",
    "\n",
    "        for _ in range(depth - 1):\n",
    "            layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "        layers.append(nn.Linear(hidden_dim, 1))\n",
    "\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def forward(self, s1: torch.Tensor, s2: torch.Tensor) -> torch.Tensor:\n",
    "        return self.network(torch.cat([s1, s2], dim=-1))\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        for layer in self.modules():\n",
    "            if type(layer) == nn.Linear:\n",
    "                nn.init.xavier_normal_(layer.weight)\n",
    "                nn.init.zeros_(layer.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(\n",
    "    model: WNLIFFNN,\n",
    "    dev_data_embedded: List[Dict[str, torch.Tensor]],\n",
    "    dev_labels: torch.Tensor,\n",
    "    eval_batch_size: int = 128,\n",
    "    device: str = \"cpu\",\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Evaluates the model on the WNLI dataset.\n",
    "\n",
    "    Inputs:\n",
    "    - model: The WNLIFFNN model\n",
    "    - dev_data_embedded: List of dictionaries containing the embedded context and disambiguation for the validation data\n",
    "    - dev_labels: List of labels for the validation data\n",
    "    - eval_batch_size: Batch size for evaluation\n",
    "    - device: Device to run the evaluation on\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    loader = get_wnli_dataloader(dev_data_embedded, dev_labels, eval_batch_size, shuffle=False)\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    avg_loss = 0\n",
    "    acc = 0\n",
    "    count = 0\n",
    "\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            l = batch[\"label\"]\n",
    "\n",
    "            s1 = batch[\"sentence1\"]\n",
    "            s2 = batch[\"sentence2\"]\n",
    "\n",
    "            logits = model(s1, s2).squeeze()\n",
    "            loss = loss_fn(logits, l)\n",
    "\n",
    "            avg_loss += loss.item()\n",
    "\n",
    "            preds = (torch.sigmoid(logits) > 0.5).float()\n",
    "            acc += (preds == l).sum()\n",
    "            count += l.shape[0]\n",
    "\n",
    "    metrics = {\n",
    "        \"loss\": avg_loss / len(loader),\n",
    "        \"accuracy\": (acc / count).item(),\n",
    "    }\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def train(\n",
    "    model: WNLIFFNN,\n",
    "    train_data_embedded: List[Dict[str, torch.Tensor]],\n",
    "    train_labels: List[str],\n",
    "    dev_data_embedded: List[Dict[str, torch.Tensor]],\n",
    "    dev_labels: List[str],\n",
    "    lr: float = 1e-3,\n",
    "    batch_size: int = 32,\n",
    "    eval_batch_size: int = 128,\n",
    "    n_epochs: int = 10,\n",
    "    device: str = \"cpu\",\n",
    "    verbose: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Runs the training loop for `n_epochs` epochs on the WNLI dataset.\n",
    "\n",
    "    Inputs:\n",
    "    - model: The WNLIFFNN model to be trained\n",
    "    - train_data_embedded: List of dictionaries containing the embedded context and disambiguation for the training data\n",
    "    - train_labels: List of labels for the training data\n",
    "    - dev_data_embedded: List of dictionaries containing the embedded context and disambiguation for the validation data\n",
    "    - dev_labels: List of labels for the validation data\n",
    "    - lr: Learning rate for the optimizer\n",
    "    - n_epochs: Number of epochs to train the model\n",
    "\n",
    "    Returns:\n",
    "    - train_losses: List of training losses for each epoch\n",
    "    - dev_metrics: List[Dict[str, float]] of validation metrics (loss, accuracy) for each epoch\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    loader = get_wnli_dataloader(train_data_embedded, train_labels, batch_size, shuffle=True)\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "    optimizer = Adam(model.parameters(), lr)\n",
    "\n",
    "    train_losses = []\n",
    "    dev_metrics = []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        e_loss = 0\n",
    "\n",
    "        for batch in loader:\n",
    "            l = batch[\"label\"]\n",
    "\n",
    "            s1 = batch[\"sentence1\"]\n",
    "            s2 = batch[\"sentence2\"]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(s1, s2).squeeze()\n",
    "            loss = loss_fn(logits, l)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            e_loss += loss.item()\n",
    "\n",
    "        e_loss /= len(loader)\n",
    "        train_losses.append(e_loss)\n",
    "\n",
    "        metrics = evaluate(model, dev_data_embedded, dev_labels, eval_batch_size, device)\n",
    "        dev_metrics.append(metrics)\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Epoch: %.d, Train Loss: %.4f, Dev Loss: %.4f, Dev Accuracy: %.4f\" % (epoch + 1, e_loss, metrics[\"loss\"], metrics[\"accuracy\"]))\n",
    "\n",
    "    return train_losses, dev_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrs = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1]\n",
    "n_epochs = [i * 10 for i in range(1, 11)]\n",
    "batch_sizes = [2 ** i for i in range(5, 10)]\n",
    "depths = [i for i in range(1, 6)]\n",
    "hidden_units = [2 ** i for i in range(6, 12)]\n",
    "\n",
    "combinations = list(product(lrs, n_epochs, batch_sizes, depths, hidden_units))\n",
    "choices = len(combinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial  1\n",
      "lr: 0.0001 batch_size: 64 epoch: 10 depth: 2 width: 128\n",
      "Dev Accuracy: 0.23943662643432617 Best Accuracy: 0.23943662643432617\n",
      "Trial  2\n",
      "lr: 1e-05 batch_size: 128 epoch: 30 depth: 1 width: 2048\n",
      "Dev Accuracy: 0.19718310236930847 Best Accuracy: 0.23943662643432617\n",
      "Trial  3\n",
      "lr: 0.001 batch_size: 128 epoch: 10 depth: 4 width: 64\n",
      "Dev Accuracy: 0.11267605423927307 Best Accuracy: 0.23943662643432617\n",
      "Trial  4\n",
      "lr: 0.001 batch_size: 512 epoch: 40 depth: 4 width: 512\n",
      "Dev Accuracy: 0.11267605423927307 Best Accuracy: 0.23943662643432617\n",
      "Trial  5\n",
      "lr: 1e-05 batch_size: 256 epoch: 50 depth: 3 width: 1024\n",
      "Dev Accuracy: 0.19718310236930847 Best Accuracy: 0.23943662643432617\n",
      "Trial  6\n",
      "lr: 0.1 batch_size: 256 epoch: 70 depth: 2 width: 64\n",
      "Dev Accuracy: 0.5492957830429077 Best Accuracy: 0.5492957830429077\n",
      "Trial  7\n",
      "lr: 0.1 batch_size: 256 epoch: 60 depth: 5 width: 2048\n",
      "Dev Accuracy: 0.4084506928920746 Best Accuracy: 0.5492957830429077\n",
      "Trial  8\n",
      "lr: 0.001 batch_size: 128 epoch: 50 depth: 1 width: 2048\n",
      "Dev Accuracy: 0.09859155118465424 Best Accuracy: 0.5492957830429077\n",
      "Trial  9\n",
      "lr: 0.001 batch_size: 32 epoch: 20 depth: 3 width: 128\n",
      "Dev Accuracy: 0.0845070406794548 Best Accuracy: 0.5492957830429077\n",
      "Trial  10\n",
      "lr: 0.001 batch_size: 32 epoch: 20 depth: 2 width: 64\n",
      "Dev Accuracy: 0.11267605423927307 Best Accuracy: 0.5492957830429077\n",
      "Trial  11\n",
      "lr: 1e-05 batch_size: 128 epoch: 20 depth: 5 width: 512\n",
      "Dev Accuracy: 0.5070422291755676 Best Accuracy: 0.5492957830429077\n",
      "Trial  12\n",
      "lr: 1e-05 batch_size: 128 epoch: 100 depth: 5 width: 512\n",
      "Dev Accuracy: 0.14084507524967194 Best Accuracy: 0.5492957830429077\n",
      "Trial  13\n",
      "lr: 1e-05 batch_size: 256 epoch: 90 depth: 5 width: 64\n",
      "Dev Accuracy: 0.4225352108478546 Best Accuracy: 0.5492957830429077\n",
      "Trial  14\n",
      "lr: 1e-05 batch_size: 256 epoch: 30 depth: 1 width: 64\n",
      "Dev Accuracy: 0.39436620473861694 Best Accuracy: 0.5492957830429077\n",
      "Trial  15\n",
      "lr: 0.001 batch_size: 512 epoch: 70 depth: 2 width: 256\n",
      "Dev Accuracy: 0.0845070406794548 Best Accuracy: 0.5492957830429077\n",
      "Trial  16\n",
      "lr: 0.1 batch_size: 512 epoch: 20 depth: 2 width: 256\n",
      "Dev Accuracy: 0.47887325286865234 Best Accuracy: 0.5492957830429077\n",
      "Trial  17\n",
      "lr: 0.0001 batch_size: 128 epoch: 60 depth: 4 width: 256\n",
      "Dev Accuracy: 0.0845070406794548 Best Accuracy: 0.5492957830429077\n",
      "Trial  18\n",
      "lr: 0.001 batch_size: 256 epoch: 40 depth: 5 width: 256\n",
      "Dev Accuracy: 0.11267605423927307 Best Accuracy: 0.5492957830429077\n",
      "Trial  19\n",
      "lr: 0.001 batch_size: 32 epoch: 70 depth: 5 width: 512\n",
      "Dev Accuracy: 0.0845070406794548 Best Accuracy: 0.5492957830429077\n",
      "Trial  20\n",
      "lr: 1e-05 batch_size: 128 epoch: 20 depth: 2 width: 64\n",
      "Dev Accuracy: 0.5211267471313477 Best Accuracy: 0.5492957830429077\n",
      "Trial  21\n",
      "lr: 0.01 batch_size: 512 epoch: 10 depth: 3 width: 512\n",
      "Dev Accuracy: 0.22535210847854614 Best Accuracy: 0.5492957830429077\n",
      "Trial  22\n",
      "lr: 0.1 batch_size: 128 epoch: 80 depth: 1 width: 128\n",
      "Dev Accuracy: 0.4647887349128723 Best Accuracy: 0.5492957830429077\n",
      "Trial  23\n",
      "lr: 0.01 batch_size: 128 epoch: 70 depth: 5 width: 1024\n",
      "Dev Accuracy: 0.2535211145877838 Best Accuracy: 0.5492957830429077\n",
      "Trial  24\n",
      "lr: 1e-05 batch_size: 512 epoch: 30 depth: 5 width: 64\n",
      "Dev Accuracy: 0.5352112650871277 Best Accuracy: 0.5492957830429077\n",
      "Trial  25\n",
      "lr: 0.1 batch_size: 64 epoch: 10 depth: 1 width: 1024\n",
      "Dev Accuracy: 0.3239436745643616 Best Accuracy: 0.5492957830429077\n",
      "Trial  26\n",
      "lr: 0.1 batch_size: 128 epoch: 90 depth: 1 width: 2048\n",
      "Dev Accuracy: 0.14084507524967194 Best Accuracy: 0.5492957830429077\n",
      "Trial  27\n",
      "lr: 0.001 batch_size: 256 epoch: 80 depth: 4 width: 2048\n",
      "Dev Accuracy: 0.09859155118465424 Best Accuracy: 0.5492957830429077\n",
      "Trial  28\n",
      "lr: 0.01 batch_size: 128 epoch: 60 depth: 3 width: 1024\n",
      "Dev Accuracy: 0.11267605423927307 Best Accuracy: 0.5492957830429077\n",
      "Trial  29\n",
      "lr: 0.1 batch_size: 512 epoch: 80 depth: 2 width: 128\n",
      "Dev Accuracy: 0.28169015049934387 Best Accuracy: 0.5492957830429077\n",
      "Trial  30\n",
      "lr: 0.0001 batch_size: 256 epoch: 30 depth: 1 width: 256\n",
      "Dev Accuracy: 0.18309858441352844 Best Accuracy: 0.5492957830429077\n",
      "Trial  31\n",
      "lr: 1e-05 batch_size: 256 epoch: 90 depth: 5 width: 2048\n",
      "Dev Accuracy: 0.07042253762483597 Best Accuracy: 0.5492957830429077\n",
      "Trial  32\n",
      "lr: 1e-05 batch_size: 64 epoch: 50 depth: 2 width: 256\n",
      "Dev Accuracy: 0.3802816867828369 Best Accuracy: 0.5492957830429077\n",
      "Trial  33\n",
      "lr: 1e-05 batch_size: 512 epoch: 100 depth: 3 width: 2048\n",
      "Dev Accuracy: 0.1267605572938919 Best Accuracy: 0.5492957830429077\n",
      "Trial  34\n",
      "lr: 1e-05 batch_size: 512 epoch: 60 depth: 2 width: 1024\n",
      "Dev Accuracy: 0.23943662643432617 Best Accuracy: 0.5492957830429077\n",
      "Trial  35\n",
      "lr: 0.1 batch_size: 128 epoch: 50 depth: 3 width: 1024\n",
      "Dev Accuracy: 0.5492957830429077 Best Accuracy: 0.5492957830429077\n",
      "Trial  36\n",
      "lr: 0.1 batch_size: 32 epoch: 100 depth: 5 width: 256\n",
      "Dev Accuracy: 0.5633803009986877 Best Accuracy: 0.5633803009986877\n",
      "Trial  37\n",
      "lr: 0.1 batch_size: 64 epoch: 60 depth: 2 width: 64\n",
      "Dev Accuracy: 0.5633803009986877 Best Accuracy: 0.5633803009986877\n",
      "Trial  38\n",
      "lr: 1e-05 batch_size: 512 epoch: 70 depth: 3 width: 128\n",
      "Dev Accuracy: 0.5352112650871277 Best Accuracy: 0.5633803009986877\n",
      "Trial  39\n",
      "lr: 0.001 batch_size: 128 epoch: 80 depth: 4 width: 512\n",
      "Dev Accuracy: 0.07042253762483597 Best Accuracy: 0.5633803009986877\n",
      "Trial  40\n",
      "lr: 0.1 batch_size: 256 epoch: 60 depth: 4 width: 512\n",
      "Dev Accuracy: 0.5633803009986877 Best Accuracy: 0.5633803009986877\n",
      "Trial  41\n",
      "lr: 0.01 batch_size: 512 epoch: 90 depth: 4 width: 128\n",
      "Dev Accuracy: 0.0845070406794548 Best Accuracy: 0.5633803009986877\n",
      "Trial  42\n",
      "lr: 1e-05 batch_size: 256 epoch: 90 depth: 4 width: 1024\n",
      "Dev Accuracy: 0.1267605572938919 Best Accuracy: 0.5633803009986877\n",
      "Trial  43\n",
      "lr: 0.001 batch_size: 256 epoch: 70 depth: 2 width: 128\n",
      "Dev Accuracy: 0.09859155118465424 Best Accuracy: 0.5633803009986877\n",
      "Trial  44\n",
      "lr: 0.0001 batch_size: 128 epoch: 20 depth: 1 width: 1024\n",
      "Dev Accuracy: 0.14084507524967194 Best Accuracy: 0.5633803009986877\n",
      "Trial  45\n",
      "lr: 0.1 batch_size: 64 epoch: 100 depth: 4 width: 64\n",
      "Dev Accuracy: 0.5633803009986877 Best Accuracy: 0.5633803009986877\n",
      "Trial  46\n",
      "lr: 0.0001 batch_size: 64 epoch: 50 depth: 3 width: 256\n",
      "Dev Accuracy: 0.0845070406794548 Best Accuracy: 0.5633803009986877\n",
      "Trial  47\n",
      "lr: 0.0001 batch_size: 512 epoch: 90 depth: 1 width: 256\n",
      "Dev Accuracy: 0.1267605572938919 Best Accuracy: 0.5633803009986877\n",
      "Trial  48\n",
      "lr: 0.0001 batch_size: 128 epoch: 50 depth: 2 width: 2048\n",
      "Dev Accuracy: 0.09859155118465424 Best Accuracy: 0.5633803009986877\n",
      "Trial  49\n",
      "lr: 0.1 batch_size: 128 epoch: 80 depth: 3 width: 512\n",
      "Dev Accuracy: 0.4225352108478546 Best Accuracy: 0.5633803009986877\n",
      "Trial  50\n",
      "lr: 0.0001 batch_size: 32 epoch: 20 depth: 3 width: 256\n",
      "Dev Accuracy: 0.11267605423927307 Best Accuracy: 0.5633803009986877\n",
      "Trial  51\n",
      "lr: 1e-05 batch_size: 128 epoch: 60 depth: 5 width: 256\n",
      "Dev Accuracy: 0.2957746386528015 Best Accuracy: 0.5633803009986877\n",
      "Trial  52\n",
      "lr: 0.01 batch_size: 256 epoch: 70 depth: 5 width: 512\n",
      "Dev Accuracy: 0.09859155118465424 Best Accuracy: 0.5633803009986877\n",
      "Trial  53\n",
      "lr: 0.001 batch_size: 128 epoch: 10 depth: 4 width: 256\n",
      "Dev Accuracy: 0.0845070406794548 Best Accuracy: 0.5633803009986877\n",
      "Trial  54\n",
      "lr: 0.1 batch_size: 512 epoch: 30 depth: 1 width: 64\n",
      "Dev Accuracy: 0.11267605423927307 Best Accuracy: 0.5633803009986877\n",
      "Trial  55\n",
      "lr: 0.01 batch_size: 256 epoch: 90 depth: 2 width: 64\n",
      "Dev Accuracy: 0.0845070406794548 Best Accuracy: 0.5633803009986877\n",
      "Trial  56\n",
      "lr: 0.01 batch_size: 256 epoch: 50 depth: 2 width: 128\n",
      "Dev Accuracy: 0.07042253762483597 Best Accuracy: 0.5633803009986877\n",
      "Trial  57\n",
      "lr: 0.0001 batch_size: 32 epoch: 70 depth: 2 width: 128\n",
      "Dev Accuracy: 0.09859155118465424 Best Accuracy: 0.5633803009986877\n",
      "Trial  58\n",
      "lr: 0.0001 batch_size: 128 epoch: 20 depth: 3 width: 128\n",
      "Dev Accuracy: 0.15492957830429077 Best Accuracy: 0.5633803009986877\n",
      "Trial  59\n",
      "lr: 1e-05 batch_size: 64 epoch: 10 depth: 2 width: 128\n",
      "Dev Accuracy: 0.5211267471313477 Best Accuracy: 0.5633803009986877\n",
      "Trial  60\n",
      "lr: 0.1 batch_size: 512 epoch: 80 depth: 2 width: 2048\n",
      "Dev Accuracy: 0.3239436745643616 Best Accuracy: 0.5633803009986877\n",
      "Trial  61\n",
      "lr: 0.001 batch_size: 64 epoch: 80 depth: 5 width: 256\n",
      "Dev Accuracy: 0.09859155118465424 Best Accuracy: 0.5633803009986877\n",
      "Trial  62\n",
      "lr: 0.001 batch_size: 32 epoch: 60 depth: 3 width: 1024\n",
      "Dev Accuracy: 0.07042253762483597 Best Accuracy: 0.5633803009986877\n",
      "Trial  63\n",
      "lr: 1e-05 batch_size: 128 epoch: 40 depth: 5 width: 512\n",
      "Dev Accuracy: 0.19718310236930847 Best Accuracy: 0.5633803009986877\n",
      "Trial  64\n",
      "lr: 0.0001 batch_size: 32 epoch: 40 depth: 3 width: 512\n",
      "Dev Accuracy: 0.11267605423927307 Best Accuracy: 0.5633803009986877\n",
      "Best Combination: {'lr': 0.1, 'epoch': 100, 'batch_size': 32, 'depth': 5, 'width': 256, 'accuracy': 0.5633803009986877}\n"
     ]
    }
   ],
   "source": [
    "best_st= {\"lr\": 0, \"epoch\": 0, \"batch_size\": 0, \"depth\": 0, \"width\": 0, \"accuracy\" : 0}\n",
    "for i in range(64):\n",
    "    print(\"Trial \", i + 1)\n",
    "    lr, epoch, batch_size, depth, width = combinations[np.random.choice(choices)]\n",
    "    print(\"lr:\", lr, \"batch_size:\", batch_size, \"epoch:\", epoch, \"depth:\", depth, \"width:\", width)\n",
    "\n",
    "    model = WNLIFFNN(input_dim=768, hidden_dim=width, depth=depth)\n",
    "    tl, dm = train(model, X_train_st, Y_train, X_dev_st, Y_dev, \n",
    "                           lr=lr, n_epochs=epoch, batch_size=batch_size, device='cpu', verbose=False)\n",
    "    \n",
    "    if dm[-1][\"accuracy\"] > best_st[\"accuracy\"]:\n",
    "        best_st[\"accuracy\"] = dm[-1][\"accuracy\"]\n",
    "        best_st[\"lr\"] = lr\n",
    "        best_st[\"epoch\"] = epoch\n",
    "        best_st[\"batch_size\"] = batch_size\n",
    "        best_st[\"depth\"] = depth\n",
    "        best_st[\"width\"] = width\n",
    "    print('Dev Accuracy:', dm[-1][\"accuracy\"], 'Best Accuracy:', best_st[\"accuracy\"])\n",
    "print('Best Combination:', best_st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial  1\n",
      "lr: 1e-05 batch_size: 32 epoch: 60 depth: 5 width: 64\n",
      "Dev Accuracy: 0.4507042169570923 Best Accuracy: 0\n",
      "Trial  2\n",
      "lr: 0.001 batch_size: 128 epoch: 80 depth: 4 width: 128\n",
      "Dev Accuracy: 0.1267605572938919 Best Accuracy: 0.4507042169570923\n",
      "Trial  3\n",
      "lr: 0.001 batch_size: 256 epoch: 10 depth: 2 width: 1024\n",
      "Dev Accuracy: 0.5633803009986877 Best Accuracy: 0.4507042169570923\n",
      "Trial  4\n",
      "lr: 0.0001 batch_size: 64 epoch: 80 depth: 3 width: 2048\n",
      "Dev Accuracy: 0.0845070406794548 Best Accuracy: 0.5633803009986877\n",
      "Trial  5\n",
      "lr: 0.0001 batch_size: 128 epoch: 10 depth: 1 width: 64\n",
      "Dev Accuracy: 0.47887325286865234 Best Accuracy: 0.5633803009986877\n",
      "Trial  6\n",
      "lr: 0.001 batch_size: 512 epoch: 60 depth: 5 width: 1024\n",
      "Dev Accuracy: 0.4507042169570923 Best Accuracy: 0.5633803009986877\n",
      "Trial  7\n",
      "lr: 1e-05 batch_size: 256 epoch: 80 depth: 3 width: 512\n",
      "Dev Accuracy: 0.4084506928920746 Best Accuracy: 0.5633803009986877\n",
      "Trial  8\n",
      "lr: 0.1 batch_size: 64 epoch: 10 depth: 4 width: 512\n",
      "Dev Accuracy: 0.5492957830429077 Best Accuracy: 0.5633803009986877\n",
      "Trial  9\n",
      "lr: 0.001 batch_size: 32 epoch: 60 depth: 4 width: 2048\n",
      "Dev Accuracy: 0.23943662643432617 Best Accuracy: 0.5633803009986877\n",
      "Trial  10\n",
      "lr: 0.001 batch_size: 32 epoch: 60 depth: 5 width: 1024\n",
      "Dev Accuracy: 0.39436620473861694 Best Accuracy: 0.5633803009986877\n",
      "Trial  11\n",
      "lr: 1e-05 batch_size: 128 epoch: 50 depth: 5 width: 64\n",
      "Dev Accuracy: 0.47887325286865234 Best Accuracy: 0.5633803009986877\n",
      "Trial  12\n",
      "lr: 0.001 batch_size: 256 epoch: 60 depth: 1 width: 2048\n",
      "Dev Accuracy: 0.26760563254356384 Best Accuracy: 0.5633803009986877\n",
      "Trial  13\n",
      "lr: 0.001 batch_size: 128 epoch: 10 depth: 4 width: 64\n",
      "Dev Accuracy: 0.4225352108478546 Best Accuracy: 0.5633803009986877\n",
      "Trial  14\n",
      "lr: 0.1 batch_size: 32 epoch: 90 depth: 1 width: 2048\n",
      "Dev Accuracy: 0.43661972880363464 Best Accuracy: 0.5633803009986877\n",
      "Trial  15\n",
      "lr: 0.01 batch_size: 64 epoch: 80 depth: 5 width: 128\n",
      "Dev Accuracy: 0.5633803009986877 Best Accuracy: 0.5633803009986877\n",
      "Trial  16\n",
      "lr: 0.001 batch_size: 32 epoch: 40 depth: 5 width: 256\n",
      "Dev Accuracy: 0.2535211145877838 Best Accuracy: 0.5633803009986877\n",
      "Trial  17\n",
      "lr: 0.01 batch_size: 64 epoch: 100 depth: 2 width: 1024\n",
      "Dev Accuracy: 0.5633803009986877 Best Accuracy: 0.5633803009986877\n",
      "Trial  18\n",
      "lr: 0.0001 batch_size: 64 epoch: 90 depth: 1 width: 256\n",
      "Dev Accuracy: 0.26760563254356384 Best Accuracy: 0.5633803009986877\n",
      "Trial  19\n",
      "lr: 0.01 batch_size: 512 epoch: 40 depth: 2 width: 2048\n",
      "Dev Accuracy: 0.43661972880363464 Best Accuracy: 0.5633803009986877\n",
      "Trial  20\n",
      "lr: 1e-05 batch_size: 128 epoch: 100 depth: 4 width: 2048\n",
      "Dev Accuracy: 0.09859155118465424 Best Accuracy: 0.5633803009986877\n",
      "Trial  21\n",
      "lr: 0.01 batch_size: 32 epoch: 60 depth: 1 width: 128\n",
      "Dev Accuracy: 0.14084507524967194 Best Accuracy: 0.5633803009986877\n",
      "Trial  22\n",
      "lr: 1e-05 batch_size: 32 epoch: 100 depth: 5 width: 64\n",
      "Dev Accuracy: 0.3239436745643616 Best Accuracy: 0.5633803009986877\n",
      "Trial  23\n",
      "lr: 1e-05 batch_size: 256 epoch: 90 depth: 1 width: 128\n",
      "Dev Accuracy: 0.5070422291755676 Best Accuracy: 0.5633803009986877\n",
      "Trial  24\n",
      "lr: 0.01 batch_size: 128 epoch: 30 depth: 3 width: 1024\n",
      "Dev Accuracy: 0.4084506928920746 Best Accuracy: 0.5633803009986877\n",
      "Trial  25\n",
      "lr: 1e-05 batch_size: 64 epoch: 10 depth: 5 width: 64\n",
      "Dev Accuracy: 0.4507042169570923 Best Accuracy: 0.5633803009986877\n",
      "Trial  26\n",
      "lr: 0.0001 batch_size: 128 epoch: 10 depth: 3 width: 2048\n",
      "Dev Accuracy: 0.35211268067359924 Best Accuracy: 0.5633803009986877\n",
      "Trial  27\n",
      "lr: 0.001 batch_size: 256 epoch: 30 depth: 2 width: 1024\n",
      "Dev Accuracy: 0.22535210847854614 Best Accuracy: 0.5633803009986877\n",
      "Trial  28\n",
      "lr: 0.01 batch_size: 128 epoch: 90 depth: 5 width: 2048\n",
      "Dev Accuracy: 0.5633803009986877 Best Accuracy: 0.5633803009986877\n",
      "Trial  29\n",
      "lr: 0.0001 batch_size: 32 epoch: 20 depth: 2 width: 256\n",
      "Dev Accuracy: 0.2957746386528015 Best Accuracy: 0.5633803009986877\n",
      "Trial  30\n",
      "lr: 0.0001 batch_size: 256 epoch: 60 depth: 1 width: 256\n",
      "Dev Accuracy: 0.4225352108478546 Best Accuracy: 0.5633803009986877\n",
      "Trial  31\n",
      "lr: 0.1 batch_size: 256 epoch: 60 depth: 3 width: 2048\n",
      "Dev Accuracy: 0.5352112650871277 Best Accuracy: 0.5633803009986877\n",
      "Trial  32\n",
      "lr: 0.0001 batch_size: 512 epoch: 50 depth: 1 width: 128\n",
      "Dev Accuracy: 0.5070422291755676 Best Accuracy: 0.5633803009986877\n",
      "Trial  33\n",
      "lr: 0.01 batch_size: 32 epoch: 70 depth: 3 width: 128\n",
      "Dev Accuracy: 0.2957746386528015 Best Accuracy: 0.5633803009986877\n",
      "Trial  34\n",
      "lr: 1e-05 batch_size: 256 epoch: 100 depth: 5 width: 256\n",
      "Dev Accuracy: 0.3239436745643616 Best Accuracy: 0.5633803009986877\n",
      "Trial  35\n",
      "lr: 0.001 batch_size: 64 epoch: 20 depth: 5 width: 2048\n",
      "Dev Accuracy: 0.5070422291755676 Best Accuracy: 0.5633803009986877\n",
      "Trial  36\n",
      "lr: 1e-05 batch_size: 64 epoch: 100 depth: 1 width: 256\n",
      "Dev Accuracy: 0.4507042169570923 Best Accuracy: 0.5633803009986877\n",
      "Trial  37\n",
      "lr: 0.0001 batch_size: 32 epoch: 60 depth: 1 width: 256\n",
      "Dev Accuracy: 0.30985915660858154 Best Accuracy: 0.5633803009986877\n",
      "Trial  38\n",
      "lr: 0.1 batch_size: 512 epoch: 10 depth: 3 width: 128\n",
      "Dev Accuracy: 0.43661972880363464 Best Accuracy: 0.5633803009986877\n",
      "Trial  39\n",
      "lr: 0.001 batch_size: 64 epoch: 60 depth: 5 width: 256\n",
      "Dev Accuracy: 0.1690140813589096 Best Accuracy: 0.5633803009986877\n",
      "Trial  40\n",
      "lr: 0.0001 batch_size: 256 epoch: 50 depth: 1 width: 64\n",
      "Dev Accuracy: 0.47887325286865234 Best Accuracy: 0.5633803009986877\n",
      "Trial  41\n",
      "lr: 0.1 batch_size: 512 epoch: 70 depth: 3 width: 64\n",
      "Dev Accuracy: 0.5633803009986877 Best Accuracy: 0.5633803009986877\n",
      "Trial  42\n",
      "lr: 0.001 batch_size: 32 epoch: 60 depth: 5 width: 256\n",
      "Dev Accuracy: 0.26760563254356384 Best Accuracy: 0.5633803009986877\n",
      "Trial  43\n",
      "lr: 1e-05 batch_size: 64 epoch: 80 depth: 4 width: 512\n",
      "Dev Accuracy: 0.19718310236930847 Best Accuracy: 0.5633803009986877\n",
      "Trial  44\n",
      "lr: 1e-05 batch_size: 256 epoch: 40 depth: 5 width: 256\n",
      "Dev Accuracy: 0.35211268067359924 Best Accuracy: 0.5633803009986877\n",
      "Trial  45\n",
      "lr: 0.001 batch_size: 32 epoch: 70 depth: 3 width: 64\n",
      "Dev Accuracy: 0.1267605572938919 Best Accuracy: 0.5633803009986877\n",
      "Trial  46\n",
      "lr: 0.001 batch_size: 512 epoch: 30 depth: 4 width: 2048\n",
      "Dev Accuracy: 0.30985915660858154 Best Accuracy: 0.5633803009986877\n",
      "Trial  47\n",
      "lr: 0.01 batch_size: 32 epoch: 30 depth: 1 width: 1024\n",
      "Dev Accuracy: 0.4225352108478546 Best Accuracy: 0.5633803009986877\n",
      "Trial  48\n",
      "lr: 1e-05 batch_size: 64 epoch: 90 depth: 5 width: 128\n",
      "Dev Accuracy: 0.49295774102211 Best Accuracy: 0.5633803009986877\n",
      "Trial  49\n",
      "lr: 0.001 batch_size: 32 epoch: 90 depth: 3 width: 2048\n",
      "Dev Accuracy: 0.2535211145877838 Best Accuracy: 0.5633803009986877\n",
      "Trial  50\n",
      "lr: 0.1 batch_size: 64 epoch: 90 depth: 5 width: 64\n",
      "Dev Accuracy: 0.43661972880363464 Best Accuracy: 0.5633803009986877\n",
      "Trial  51\n",
      "lr: 0.001 batch_size: 128 epoch: 10 depth: 4 width: 1024\n",
      "Dev Accuracy: 0.5633803009986877 Best Accuracy: 0.5633803009986877\n",
      "Trial  52\n",
      "lr: 0.1 batch_size: 32 epoch: 100 depth: 1 width: 512\n",
      "Dev Accuracy: 0.5633803009986877 Best Accuracy: 0.5633803009986877\n",
      "Trial  53\n",
      "lr: 0.1 batch_size: 32 epoch: 60 depth: 4 width: 64\n",
      "Dev Accuracy: 0.43661972880363464 Best Accuracy: 0.5633803009986877\n",
      "Trial  54\n",
      "lr: 0.1 batch_size: 256 epoch: 40 depth: 4 width: 128\n",
      "Dev Accuracy: 0.577464759349823 Best Accuracy: 0.5633803009986877\n",
      "Trial  55\n",
      "lr: 1e-05 batch_size: 256 epoch: 20 depth: 5 width: 128\n",
      "Dev Accuracy: 0.47887325286865234 Best Accuracy: 0.577464759349823\n",
      "Trial  56\n",
      "lr: 1e-05 batch_size: 64 epoch: 90 depth: 5 width: 2048\n",
      "Dev Accuracy: 0.11267605423927307 Best Accuracy: 0.577464759349823\n",
      "Trial  57\n",
      "lr: 1e-05 batch_size: 64 epoch: 20 depth: 4 width: 2048\n",
      "Dev Accuracy: 0.4225352108478546 Best Accuracy: 0.577464759349823\n",
      "Trial  58\n",
      "lr: 1e-05 batch_size: 512 epoch: 20 depth: 4 width: 256\n",
      "Dev Accuracy: 0.49295774102211 Best Accuracy: 0.577464759349823\n",
      "Trial  59\n",
      "lr: 0.001 batch_size: 64 epoch: 90 depth: 3 width: 64\n",
      "Dev Accuracy: 0.09859155118465424 Best Accuracy: 0.577464759349823\n",
      "Trial  60\n",
      "lr: 1e-05 batch_size: 128 epoch: 80 depth: 4 width: 128\n",
      "Dev Accuracy: 0.5211267471313477 Best Accuracy: 0.577464759349823\n",
      "Trial  61\n",
      "lr: 0.01 batch_size: 64 epoch: 50 depth: 2 width: 256\n",
      "Dev Accuracy: 0.5633803009986877 Best Accuracy: 0.577464759349823\n",
      "Trial  62\n",
      "lr: 1e-05 batch_size: 32 epoch: 90 depth: 3 width: 1024\n",
      "Dev Accuracy: 0.1267605572938919 Best Accuracy: 0.577464759349823\n",
      "Trial  63\n",
      "lr: 0.01 batch_size: 32 epoch: 60 depth: 5 width: 128\n",
      "Dev Accuracy: 0.5633803009986877 Best Accuracy: 0.577464759349823\n",
      "Trial  64\n",
      "lr: 0.0001 batch_size: 256 epoch: 40 depth: 4 width: 256\n",
      "Dev Accuracy: 0.23943662643432617 Best Accuracy: 0.577464759349823\n",
      "Best Combination: {'lr': 0.1, 'epoch': 40, 'batch_size': 256, 'depth': 4, 'width': 128, 'accuracy': 0.577464759349823}\n"
     ]
    }
   ],
   "source": [
    "best_glove= {\"lr\": 0, \"epoch\": 0, \"batch_size\": 0, \"depth\": 0, \"width\": 0, \"accuracy\" : 0}\n",
    "for i in range(64):\n",
    "    print(\"Trial \", i + 1)\n",
    "    lr, epoch, batch_size, depth, width = combinations[np.random.choice(choices)]\n",
    "    print(\"lr:\", lr, \"batch_size:\", batch_size, \"epoch:\", epoch, \"depth:\", depth, \"width:\", width)\n",
    "\n",
    "    model = WNLIFFNN(input_dim=50, hidden_dim=width, depth=depth)\n",
    "    tl, dm = train(model, X_train_glove, Y_train, X_dev_glove, Y_dev, \n",
    "                           lr=lr, n_epochs=epoch, batch_size=batch_size, device='cpu', verbose=False)\n",
    "    \n",
    "    print('Dev Accuracy:', dm[-1][\"accuracy\"], 'Best Accuracy:', best_glove[\"accuracy\"])\n",
    "    if dm[-1][\"accuracy\"] > best_glove[\"accuracy\"]:\n",
    "        best_glove[\"accuracy\"] = dm[-1][\"accuracy\"]\n",
    "        best_glove[\"lr\"] = lr\n",
    "        best_glove[\"epoch\"] = epoch\n",
    "        best_glove[\"batch_size\"] = batch_size\n",
    "        best_glove[\"depth\"] = depth\n",
    "        best_glove[\"width\"] = width\n",
    "print('Best Combination:', best_glove)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cse446",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
